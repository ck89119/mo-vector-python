import os

from mo_vector.client import MoVectorClient
from sentence_transformers import SentenceTransformer
from dotenv import load_dotenv

# Step 1. Initialize embedding model
print("Downloading and loading the embedding model...")
os.environ['HTTP_PROXY'] = 'http://127.0.0.1:1089'
os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:1089'
embed_model = SentenceTransformer("sentence-transformers/msmarco-MiniLM-L12-cos-v5", trust_remote_code=True)
embed_model_dims = embed_model.get_sentence_embedding_dimension()


def text_to_embedding(text):
    """Generates vector embeddings for the given text."""
    embedding = embed_model.encode(text)
    return embedding.tolist()


# Step 2. Initialize MoVectorClient instance
load_dotenv()

username = os.environ['USERNAME'],
password = os.environ['PASSWORD'],
host = os.environ['HOST'],
port = os.environ['PORT'],
database = os.environ['DATABASE'],
connection_string = f"mysql+pymysql://{username[0]}:{password[0]}@{host[0]}:{int(port[0])}/{database[0]}"

vector_store = MoVectorClient(
    # The table which will store the vector data.
    table_name='embedded_documents',
    connection_string=connection_string,
    # The dimension of the vector generated by the embedding model.
    vector_dimension=embed_model_dims,
    # Determine whether to recreate the table if it already exists.
    drop_existing_table=True,
)

# Step 3. Bulk insert objects and their embeddings

documents = [
    {
        "id": "f8e7dee2-63b6-42f1-8b60-2d46710c1971",
        "text": "dog",
        "embedding": text_to_embedding("dog"),
        "metadata": {"category": "animal"},
    },
    {
        "id": "f8e7dee2-63b6-42f1-8b60-2d46710c1972",
        "text": "aa dog",
        "embedding": text_to_embedding("aa dog"),
        "metadata": {"category": "animal"},
    },
    {
        "id": "f8e7dee2-63b6-42f1-8b60-2d46710c1973",
        "text": "bb dog",
        "embedding": text_to_embedding("bb dog"),
        "metadata": {"category": "animal"},
    },
    {
        "id": "8dde1fbc-2522-4ca2-aedf-5dcb2966d1c6",
        "text": "fish",
        "embedding": text_to_embedding("fish"),
        "metadata": {"category": "animal"},
    },
    {
        "id": "e4991349-d00b-485c-a481-f61695f2b5ae",
        "text": "tree",
        "embedding": text_to_embedding("tree"),
        "metadata": {"category": "plant"},
    },
]

vector_store.insert(
    ids=[doc["id"] for doc in documents],
    texts=[doc["text"] for doc in documents],
    embeddings=[doc["embedding"] for doc in documents],
    metadatas=[doc["metadata"] for doc in documents],
)


# Step 4. Perform vector search to find the most semantically similar documents to the query.
def print_result(query, result):
    print(f"Search result (\"{query}\"):")
    for r in result:
        print(f"- text: \"{r.document}\", distance: {r.distance}")


query = "a swimming animal"
query_embedding = text_to_embedding(query)
search_result = vector_store.query(query_embedding, k=3)
print_result(query, search_result)
search_result = vector_store.query(query_embedding, k=3, filter={"category": "\"animal\""})
print_result(query, search_result)


# Step 5. mix query
def print_mix_result(query, keywords, result):
    print(f"Search result (\"{query}, {keywords}\"):")
    for r in result:
        print(f"- text: \"{r[1]}\", score: {r[0]}")


vector_store.create_full_text_index()
keywords = ["dog"]
# rrf
rerank_option_rrf = {"rerank_type": "RRF", "rank_value": 60}
search_result = vector_store.mix_query(query_embedding, keywords, rerank_option_rrf, k=3)
print_mix_result(query, keywords, search_result)
# weighted
rerank_option_weighted = {"rerank_type": "WeightedRank", "weighted_score": [0.8, 0.2], "rerank_score_threshold": 1}
search_result = vector_store.mix_query(query_embedding, keywords, rerank_option_weighted, k=3)
print_mix_result(query, keywords, search_result)

# Step 6. delete
vector_store.delete(ids=[doc["id"] for doc in documents])
# vector_store.delete(ids=[doc["id"] for doc in documents], filter={"category": "\"plant\""})
