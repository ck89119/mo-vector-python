import os

from mo_vector.client import MoVectorClient
from sentence_transformers import SentenceTransformer
from dotenv import load_dotenv

# Step 1. Initialize embedding model
print("Downloading and loading the embedding model...")
os.environ['HTTP_PROXY'] = 'http://127.0.0.1:1089'
os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:1089'
embed_model = SentenceTransformer("sentence-transformers/msmarco-MiniLM-L12-cos-v5", trust_remote_code=True)
embed_model_dims = embed_model.get_sentence_embedding_dimension()


def text_to_embedding(text):
    """Generates vector embeddings for the given text."""
    embedding = embed_model.encode(text)
    return embedding.tolist()


# Step 2. Initialize MoVectorClient instance
load_dotenv()

username = os.environ['USERNAME'],
password = os.environ['PASSWORD'],
host = os.environ['HOST'],
port = os.environ['PORT'],
database = os.environ['DATABASE'],
connection_string = f"mysql+pymysql://{username[0]}:{password[0]}@{host[0]}:{int(port[0])}/{database[0]}"

vector_store = MoVectorClient(
    # The table which will store the vector data.
    table_name='embedded_documents',
    connection_string=connection_string,
    # The dimension of the vector generated by the embedding model.
    vector_dimension=embed_model_dims,
    # Determine whether to recreate the table if it already exists.
    drop_existing_table=True,
)

# Step 3. Bulk insert objects and their embeddings

documents = [
    {
        "id": "f8e7dee2-63b6-42f1-8b60-2d46710c1971",
        "text": "dog",
        "embedding": text_to_embedding("dog"),
        "metadata": {"category": "animal"},
    },
    {
        "id": "f8e7dee2-63b6-42f1-8b60-2d46710c1972",
        "text": "hot dog",
        "embedding": text_to_embedding("hot dog"),
        "metadata": {"category": "food"},
    },
    {
        "id": "8dde1fbc-2522-4ca2-aedf-5dcb2966d1c6",
        "text": "fish",
        "embedding": text_to_embedding("fish"),
        "metadata": {"category": "animal"},
    },
    {
        "id": "e4991349-d00b-485c-a481-f61695f2b5ae",
        "text": "tree",
        "embedding": text_to_embedding("tree"),
        "metadata": {"category": "plant"},
    },
]

vector_store.insert(
    ids=[doc["id"] for doc in documents],
    texts=[doc["text"] for doc in documents],
    embeddings=[doc["embedding"] for doc in documents],
    metadatas=[doc["metadata"] for doc in documents],
)


# Step 4. Perform vector search to find the most semantically similar documents to the query.
def print_result(query, result):
    print(f"Search result (\"{query}\"):")
    for r in result:
        print(f"- text: \"{r.document}\", distance: {r.distance}")
    print("-----------------------------")


query = "a swimming animal"
query_embedding = text_to_embedding(query)
print(f"vector query")
search_result = vector_store.query(query_embedding, k=3)
print_result(query, search_result)
print(f"vector query with meta filter")
search_result = vector_store.query(query_embedding, k=3, filter={"category": "\"animal\""})
print_result(query, search_result)
print(f"vector query, distance in range [1.0, 1.2]")
search_result = vector_store.query(query_embedding, k=3, dis_lower_bound=1.0, dis_upper_bound=1.2)
print_result(query, search_result)

query1 = "a plant"
query_embedding1 = text_to_embedding(query1)
print(f"batch vector query")
search_results = vector_store.batch_query([query_embedding, query_embedding1], k=3)
for q, search_result in zip([query, query1], search_results):
    print_result(q, search_result)


# Step 5. full text query
def print_full_text_result(keywords, result):
    print(f"Search result (keywords: {keywords}\"):")
    for r in result:
        print(f"- text: \"{r.document}\", score: {r.distance}")
    print("-----------------------------")


vector_store.create_full_text_index()
keywords = ["dog"]
print(f"full text query")
search_result = vector_store.full_text_query(keywords, k=3)
print_full_text_result(keywords, search_result)
print(f"full text query with meta filter")
search_result = vector_store.full_text_query(keywords, k=3, filter={"category": "\"animal\""})
print_full_text_result(keywords, search_result)


# Step 6. mix query
def print_mix_result(query, keywords, result):
    print(f"Search result (query: \"{query}, keywords: {keywords}\"):")
    for r in result:
        print(f"- text: \"{r[1]}\", score: {r[0]}")
    print("-----------------------------")


# rrf
print(f"mix query with rrf")
rerank_option_rrf = {"rerank_type": "RRF", "rank_value": 60}
search_result = vector_store.mix_query(query_embedding, keywords, rerank_option_rrf, k=3)
print_mix_result(query, keywords, search_result)
# weighted
print(f"mix query with weighted")
rerank_option_weighted = {"rerank_type": "WeightedRank", "weighted_score": [0.8, 0.2], "rerank_score_threshold": 1}
search_result = vector_store.mix_query(query_embedding, keywords, rerank_option_weighted, k=3)
print_mix_result(query, keywords, search_result)


# Step 7. delete
vector_store.delete(ids=[doc["id"] for doc in documents])
